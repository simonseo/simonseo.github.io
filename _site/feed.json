{
    "version": "https://jsonfeed.org/version/1",
    "title": "image synthesis",
    "home_page_url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/course/16-726-sp23/projects/myunggus",
    "feed_url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/feed.json",
    "description": "coursework by myunggus",
    "icon": "http://localhost:4000/course/16-726-sp23/projects/myunggus/assets/images/icon-512.png",
    "favicon": "http://localhost:4000/course/16-726-sp23/projects/myunggus/favicon.ico",
    "expired": false,
    "items": [
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/conv-approx-non-line-of-sight/",
            "title": "Convolutional Approximjations to the General Non-Line-of-Sight Imaging Operator",
            "content_text": "\n\nMain contributions of this work\nThis paper achieves computational advancements in non-line-of-sight (NLOS) imaging in two ways: first by changing the solution to an efficient deconvolution algorithm, and secondly by applying iterative methods with priors. It also provides theoretical justification for a technique that researchers have been using only with an intuitive understanding. All of this is possible by approximating the measurement-backprojection pipeline of albedo as a convolution. \\(\\newcommand{\\vect}[1]{\\boldsymbol{#1}}\\) \\(\\text{argmin}_{\\vect{\\rho}} \\Vert \\vect{i}- \\vect{A \\rho} \\Vert\\) would give us the the original albedo \\(\\vect{\\rho}\\) but this method is difficult to use due to the sheer size of the measurement matrix \\(\\vect{A}\\) making it take very long to scan and computationally intractable. However, modelling the\n\nExtensions of the idea\nThis theory could be easily applied to highways to model traffic around a vehicle without having direct sight to avoid possible collisions. This technology would also be helpful at an intersection, drivers in smaller cars often cannot see traffic lights or other traffic information if there is a large truck in front. Since the Gram is a convolution operator, can we use it to design optical convolutional filters?\n\n\n\n\n\n",
            "content_html": "<div style=\"text-align:center\"><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/eMLzToMqleU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen=\"\"></iframe></div>\n\n<h2 id=\"main-contributions-of-this-work\">Main contributions of this work</h2>\n<p>This paper achieves computational advancements in non-line-of-sight (NLOS) imaging in two ways: first by changing the solution to an efficient deconvolution algorithm, and secondly by applying iterative methods with priors. It also provides theoretical justification for a technique that researchers have been using only with an intuitive understanding. All of this is possible by approximating the measurement-backprojection pipeline of albedo as a convolution. \\(\\newcommand{\\vect}[1]{\\boldsymbol{#1}}\\) \\(\\text{argmin}_{\\vect{\\rho}} \\Vert \\vect{i}- \\vect{A \\rho} \\Vert\\) would give us the the original albedo \\(\\vect{\\rho}\\) but this method is difficult to use due to the sheer size of the measurement matrix \\(\\vect{A}\\) making it take very long to scan and computationally intractable. However, modelling the</p>\n\n<h2 id=\"extensions-of-the-idea\">Extensions of the idea</h2>\n<p>This theory could be easily applied to highways to model traffic around a vehicle without having direct sight to avoid possible collisions. This technology would also be helpful at an intersection, drivers in smaller cars often cannot see traffic lights or other traffic information if there is a large truck in front. Since the Gram is a convolution operator, can we use it to design optical convolutional filters?</p>\n\n<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n\n<script id=\"MathJax-script\" async=\"\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/conv-approx-non-line-of-sight/",
            "date_published": "2023-03-22T00:00:00-04:00",
            "date_modified": "2023-03-22T00:00:00-04:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj3/",
            "title": "When Cats meet GANs",
            "content_text": "We will talk about Deep Convolutional GANs (DCGAN) and CycleGANs.\n\nGoal\n\n\n  Generate cat pictures from a randomly sampled noise vector in the latent/feature space.\n  Translate between images of two different types of cats (grumpy and Russian Blue)\n  Translate between images of apples and images of oranges\n\n\n1. Deep Convolutional GAN\n\nDCGAN is just GAN but with convolutional layers.\n\nDeep Convolutional Discriminator\n\n\nThe above image (taken from course website) describes the layers in the discriminator but we need to determine the sizes of the kernel, stride, and padding. We downsample the image by a factor of two using the following formula.\n\n\n$$\n(O \\times S)+(K-S) = (I+2P) \\\\ \nO = I /2 \n$$\n\n\nWhere O is output size, I is input size, K is kernel size, S is stride, and P is padding.\n\nPadding\nWe are given kernel size K=4, stride S=2. When S is equal to the downsampling factor, the actual values of O and I become irrelevant and we get P=1 for all layers except conv5, where we don’t need padding nor a stride if we use a kernel size K=4.\n\nDeep Convolutional Generator Up-convolution\n\n\n\nThe first layer of the generator is given input of size 1x1 which needs to be quadrupled to 4x4. Through multiple experiments, I found that using a kernel size K=5, stride S=1, padding P=2 after scaling the image to the desired size made the training more stable.\n\nPreprocess Augmentation and Differentiable Augmentation\n\nI used all three DiffAug policies color,translation,cutout and in deluxe data augmentation, I used RandomCrop, RandomHorizontalFlip, RandomRotation transforms on top of the basic data augmentation transforms.\n\nI applied DiffAug to both fake and real images any time the images needed to go through the discriminator. I also called .detach() method on fake images only when training the discriminator to fix the generator.\n\nVisual Comparison\nvanilla gan grumpy cat basic: \n \nvanilla gan grumpy cat basic diffaug:\n \nvanilla gan grumpy cat deluxe: \n \nvanilla gan grumpy cat deluxe diffaug:\n\n\nThe effects that I can observe from running 6400 iterations with these different settings are:\n\n  Data augmentation helps. Applying either deluxe or diffaug significantly enhances the image quality.\n  diffaug seems to work better than plain deluxe. Perhaps because the effect of diffaug is only applied to the discriminator. With the deluxe option, the generator must learn to generalize a broader range of data, which may have been difficult with just 6400 iterations.\n  In the case that diffaug was applied, the deluxe augmentations allow more variance in the output images but the results are not as clean as the basic augmented output. Running for longer may have produced a different outcome.\n\n\nLoss graph comparison\nThe generator’s loss for cases where DiffAug is applied is consistently lower than when it is not.\nThe loss when using deluxe augmentation is slightly lower but not significantly.\nThis trend is exactly the mirror opposite for discriminator losses. \nThis demonstrates that augmentation helps the network learn better.\n\nGenerator Loss\n\nDiscriminator Loss\n\n\nImprovement over time\nIn the beginning, the generated images have “aliasing” effects. Therea are horizontal and vertical lines that are often noticeable in low quality JPEG- or PCA- compressed images. However, the overall structure already exists and is quite discernible. As we iterate more, the aliasing effect goes away and higher frequency detail is improved.\n\niteration 200:\n\niteration 3200:\n\niteration 6400:\n\n\n2. CycleGAN\nThe domain of the CycleGAN Generator is different from that of vanilla GAN. Vanilla GAN samples from noise but CycleGAN translates an image to an image. \n\n\nAgain, the above image is taken from course website. The discriminator of CycleGAN is expands on that of vanilla GAN by using Patch Discriminator. This discriminator divides the output image into four patches and decides how realistic each patch is. This method forces detail to be consistent across different areas of the image, instead of, for example, having very realistic features in just one area.\n\nResults: Grumpy ⇄ Russian Blue\nEffect of cycle consistency loss\nThe effect of cycle consistency loss is not very pronounced for grumpy to russian blue, but the results of russian blue to grumpy show the effect a little bit. In the images generated without using cycle consistency loss, the russian blue cats are definitely unlike grumpy cats, but not necessarily the best representation of russian blue cats. In the images generated using cycle consistency loss, the russian blue cats are more like russian blue cats.\n\nAt iteration 1000 without cycle consistency loss:\n\n\nAt iteration 1000 with cycle consistency loss:\n\n\n\nEffect of DC vs Patch discriminator\nThe most notable difference in the effect of the two discriminators can be seen in the cats’ eyes. In the images generated with a DC discriminator, the overall structure matches the target cat, but detail in the eyes are not very expressive. On the other hand, using patch discriminator enhances local detail like the eyes.\n\nWith DC Discriminator:\n\n\nWith Patch Discriminator:\n\n\n\nResults: Apple ⇄ Orange\nEffect of cycle consistency loss\nThis one is funny. Without cycle consistency loss, generated oranges are ooooooranggggeeeee but not orange. The generated apples scream apppppllplplllleeee but are not apples. With cycle consistency loss, oranges are more like oranges and apples are more like apples. The reason could be attributed to the fact that it’s hard to generate an apple from an ooooooranggggeeeee, so the network is regularized to not overshoot the optimization and instead just produce an orange.\n\nAt iteration 1000 without cycle consistency loss:\n\n\nAt iteration 1000 with cycle consistency loss:\n\n\n\nEffect of DC vs Patch discriminator\nThe difference here is hard to tell. The original images are not preprocessed enough (though the cat images were) for there to be meaningful difference in the local texture.\n\nWith DC Discriminator:\n\n\n\nWith Patch Discriminator:\n\n\n\nFinal results\nUsing cycle consistency loss, patch discriminator, 10,000 iterations\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
            "content_html": "<p>We will talk about Deep Convolutional GANs (DCGAN) and CycleGANs.</p>\n\n<h2 id=\"goal\">Goal</h2>\n\n<ol>\n  <li>Generate cat pictures from a randomly sampled noise vector in the latent/feature space.</li>\n  <li>Translate between images of two different types of cats (grumpy and Russian Blue)</li>\n  <li>Translate between images of apples and images of oranges</li>\n</ol>\n\n<h2 id=\"1-deep-convolutional-gan\">1. Deep Convolutional GAN</h2>\n\n<p>DCGAN is just GAN but with convolutional layers.</p>\n\n<h3 id=\"deep-convolutional-discriminator\">Deep Convolutional Discriminator</h3>\n<p><img src=\"data/DCGAN discriminator_architecture.png\" alt=\"dcgan discriminator architecture\" /></p>\n\n<p>The above image (taken from course website) describes the layers in the discriminator but we need to determine the sizes of the kernel, stride, and padding. We downsample the image by a factor of two using the following formula.</p>\n\n<div>\n$$\n(O \\times S)+(K-S) = (I+2P) \\\\ \nO = I /2 \n$$\n</div>\n\n<p>Where O is output size, I is input size, K is kernel size, S is stride, and P is padding.</p>\n\n<h5 id=\"padding\">Padding</h5>\n<p>We are given kernel size K=4, stride S=2. When S is equal to the downsampling factor, the actual values of O and I become irrelevant and we get P=1 for all layers except <code class=\"language-plaintext highlighter-rouge\">conv5</code>, where we don’t need padding nor a stride if we use a kernel size K=4.</p>\n\n<h3 id=\"deep-convolutional-generator-up-convolution\">Deep Convolutional Generator Up-convolution</h3>\n\n<p><img src=\"data/DCGAN%20generator_architecture.png\" alt=\"generator architecture\" /></p>\n\n<p>The first layer of the generator is given input of size 1x1 which needs to be quadrupled to 4x4. Through multiple experiments, I found that using a kernel size K=5, stride S=1, padding P=2 after scaling the image to the desired size made the training more stable.</p>\n\n<h3 id=\"preprocess-augmentation-and-differentiable-augmentation\">Preprocess Augmentation and Differentiable Augmentation</h3>\n\n<p>I used all three DiffAug policies <code class=\"language-plaintext highlighter-rouge\">color,translation,cutout</code> and in deluxe data augmentation, I used <code class=\"language-plaintext highlighter-rouge\">RandomCrop</code>, <code class=\"language-plaintext highlighter-rouge\">RandomHorizontalFlip</code>, <code class=\"language-plaintext highlighter-rouge\">RandomRotation</code> transforms on top of the basic data augmentation transforms.</p>\n\n<p>I applied DiffAug to both fake and real images any time the images needed to go through the discriminator. I also called <code class=\"language-plaintext highlighter-rouge\">.detach()</code> method on fake images only when training the discriminator to fix the generator.</p>\n\n<h5 id=\"visual-comparison\">Visual Comparison</h5>\n<p>vanilla gan grumpy cat basic: \n<img src=\"data/vanilla/vanilla%20grumpy%20basic%206400.png\" alt=\"vanilla gan grumpy cat basic \" /> \n<strong>vanilla gan grumpy cat basic diffaug:</strong>\n<img src=\"data/vanilla/vanilla%20grumpy%20basic%20diffaug%206400.png\" alt=\"vanilla gan grumpy cat basic diffaug\" /> \nvanilla gan grumpy cat deluxe: \n<img src=\"data/vanilla/vanilla%20grumpy%20deluxe%206400.png\" alt=\"vanilla gan grumpy cat deluxe \" /> \n<strong>vanilla gan grumpy cat deluxe diffaug:</strong>\n<img src=\"data/vanilla/vanilla%20grumpy%20deluxe%20diffaug%206400.png\" alt=\"vanilla gan grumpy cat deluxe diffaug\" /></p>\n\n<p>The effects that I can observe from running 6400 iterations with these different settings are:</p>\n<ul>\n  <li>Data augmentation helps. Applying either <code class=\"language-plaintext highlighter-rouge\">deluxe</code> or <code class=\"language-plaintext highlighter-rouge\">diffaug</code> significantly enhances the image quality.</li>\n  <li><code class=\"language-plaintext highlighter-rouge\">diffaug</code> seems to work better than plain <code class=\"language-plaintext highlighter-rouge\">deluxe</code>. Perhaps because the effect of <code class=\"language-plaintext highlighter-rouge\">diffaug</code> is only applied to the discriminator. With the <code class=\"language-plaintext highlighter-rouge\">deluxe</code> option, the generator must learn to generalize a broader range of data, which may have been difficult with just 6400 iterations.</li>\n  <li>In the case that <code class=\"language-plaintext highlighter-rouge\">diffaug</code> was applied, the deluxe augmentations allow more variance in the output images but the results are not as clean as the basic augmented output. Running for longer may have produced a different outcome.</li>\n</ul>\n\n<h3 id=\"loss-graph-comparison\">Loss graph comparison</h3>\n<p>The generator’s loss for cases where DiffAug is applied is consistently lower than when it is not.\nThe loss when using deluxe augmentation is slightly lower but not significantly.\nThis trend is exactly the mirror opposite for discriminator losses. \nThis demonstrates that augmentation helps the network learn better.</p>\n\n<h5 id=\"generator-loss\">Generator Loss</h5>\n<p><img src=\"data/DCGAN%20Generator%20loss.png\" alt=\"Loss graph\" /></p>\n<h5 id=\"discriminator-loss\">Discriminator Loss</h5>\n<p><img src=\"data/DCGAN%20Discriminator%20loss.png\" alt=\"Loss graph\" /></p>\n\n<h3 id=\"improvement-over-time\">Improvement over time</h3>\n<p>In the beginning, the generated images have “aliasing” effects. Therea are horizontal and vertical lines that are often noticeable in low quality JPEG- or PCA- compressed images. However, the overall structure already exists and is quite discernible. As we iterate more, the aliasing effect goes away and higher frequency detail is improved.</p>\n\n<p><strong>iteration 200:</strong>\n<img src=\"data/vanilla/vanilla%20grumpy%20deluxe%20diffaug%200200.png\" alt=\"Alt text\" />\n<strong>iteration 3200:</strong>\n<img src=\"data/vanilla/vanilla%20grumpy%20deluxe%20diffaug%203200.png\" alt=\"Alt text\" />\n<strong>iteration 6400:</strong>\n<img src=\"data/vanilla/vanilla%20grumpy%20deluxe%20diffaug%206400.png\" alt=\"Alt text\" /></p>\n\n<h2 id=\"2-cyclegan\">2. CycleGAN</h2>\n<p>The domain of the CycleGAN Generator is different from that of vanilla GAN. Vanilla GAN samples from noise but CycleGAN translates an image to an image. \n<img src=\"data/cyclegan_generator_architecture.png\" alt=\"Alt text\" /></p>\n\n<p>Again, the above image is taken from course website. The discriminator of CycleGAN is expands on that of vanilla GAN by using Patch Discriminator. This discriminator divides the output image into four patches and decides how realistic each patch is. This method forces detail to be consistent across different areas of the image, instead of, for example, having very realistic features in just one area.</p>\n\n<h3 id=\"results-grumpy--russian-blue\">Results: Grumpy ⇄ Russian Blue</h3>\n<h5 id=\"effect-of-cycle-consistency-loss\">Effect of cycle consistency loss</h5>\n<p>The effect of cycle consistency loss is not very pronounced for <code class=\"language-plaintext highlighter-rouge\">grumpy to russian blue</code>, but the results of <code class=\"language-plaintext highlighter-rouge\">russian blue to grumpy</code> show the effect a little bit. In the images generated without using cycle consistency loss, the russian blue cats are definitely <em>unlike</em> grumpy cats, but not necessarily the best representation of russian blue cats. In the images generated using cycle consistency loss, the russian blue cats are more <em>like</em> russian blue cats.</p>\n\n<p><strong>At iteration 1000 <em>without</em> cycle consistency loss:</strong>\n<img src=\"data/cyclegan/cat%20naive-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/cat%20naive-001000-Y-X.png\" alt=\"Alt text\" />\n<strong>At iteration 1000 <em>with</em> cycle consistency loss:</strong>\n<img src=\"data/cyclegan/cat%20cycle%20consistency-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/cat%20cycle%20consistency-001000-Y-X.png\" alt=\"Alt text\" /></p>\n\n<h5 id=\"effect-of-dc-vs-patch-discriminator\">Effect of DC vs Patch discriminator</h5>\n<p>The most notable difference in the effect of the two discriminators can be seen in the cats’ eyes. In the images generated with a DC discriminator, the overall structure matches the target cat, but detail in the eyes are not very expressive. On the other hand, using patch discriminator enhances local detail like the eyes.</p>\n\n<p><strong>With DC Discriminator:</strong>\n<img src=\"data/cyclegan/cat%20dc-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/cat%20dc-001000-Y-X.png\" alt=\"Alt text\" />\n<strong>With Patch Discriminator:</strong>\n<img src=\"data/cyclegan/cat%20cycle%20consistency-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/cat%20cycle%20consistency-001000-Y-X.png\" alt=\"Alt text\" /></p>\n\n<h3 id=\"results-apple--orange\">Results: Apple ⇄ Orange</h3>\n<h5 id=\"effect-of-cycle-consistency-loss-1\">Effect of cycle consistency loss</h5>\n<p>This one is funny. Without cycle consistency loss, generated oranges are <em>ooooooranggggeeeee</em> but not orange. The generated apples scream <em>apppppllplplllleeee</em> but are not apples. With cycle consistency loss, oranges are more like oranges and apples are more like apples. The reason could be attributed to the fact that it’s hard to generate an apple from an <em>ooooooranggggeeeee</em>, so the network is regularized to not overshoot the optimization and instead just produce an <em>orange</em>.</p>\n\n<p><strong>At iteration 1000 <em>without</em> cycle consistency loss:</strong>\n<img src=\"data/cyclegan/fruit%20naive-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/fruit%20naive-001000-Y-X.png\" alt=\"Alt text\" />\n<strong>At iteration 1000 <em>with</em> cycle consistency loss:</strong>\n<img src=\"data/cyclegan/fruit%20patch-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/fruit%20patch-001000-Y-X.png\" alt=\"Alt text\" /></p>\n\n<h5 id=\"effect-of-dc-vs-patch-discriminator-1\">Effect of DC vs Patch discriminator</h5>\n<p>The difference here is hard to tell. The original images are not preprocessed enough (though the cat images were) for there to be meaningful difference in the local texture.</p>\n\n<p><strong>With DC Discriminator:</strong>\n<img src=\"data/cyclegan/fruit%20dc-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/fruit%20dc-001000-Y-X.png\" alt=\"Alt text\" /></p>\n\n<p><strong>With Patch Discriminator:</strong>\n<img src=\"data/cyclegan/fruit%20patch-001000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/fruit%20patch-001000-Y-X.png\" alt=\"Alt text\" /></p>\n\n<h3 id=\"final-results\">Final results</h3>\n<p>Using cycle consistency loss, patch discriminator, 10,000 iterations</p>\n\n<p><img src=\"data/cyclegan/cat%20cycle%20consistency-010000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/cat%20cycle%20consistency-010000-Y-X.png\" alt=\"Alt text\" /></p>\n\n<p><img src=\"data/cyclegan/fruit-010000-X-Y.png\" alt=\"Alt text\" />\n<img src=\"data/cyclegan/fruit-010000-Y-X.png\" alt=\"Alt text\" /></p>\n\n<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n\n<script id=\"MathJax-script\" async=\"\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n\n<!-- <script type=\"text/javascript\" src=\"/course/16-726-sp23/projects/myunggus/assets/js/MathJax/MathJax.js\"></script> -->\n<!-- <script type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js\"></script> -->\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj3/",
            "date_published": "2023-03-21T00:00:00-04:00",
            "date_modified": "2023-03-21T00:00:00-04:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/spad/",
            "title": " Asynchronous Single-Photon 3D Imaging",
            "content_text": "A.K.A. How to deal with ambient light when using SPAD as a 3D camera\nMost use cases of SPAD as a 3D camera comes with an inherent problem: ambient light. The 10m-10km measurement range of SPAD cameras means that most uses will be outdoors where ambient light is difficult to control. This paper tackles that by averaging out the ambient light. In contrast to the time of flight graph of a photon to the actual measurement target, which appears as a single high frequency mode in the time-intensity graph, ambient light is a flat low frequency line that stays constant. Because a single SPAD diode can only detect a single photon every so often, the ambient light has a high chance of hitting the diodes and leads to “pileup”. This paper fixes the issue by differing the timestamp at which a laser pulse transmits and one at which a SPAD sensor starts acquiring photons. This way, the pile of ambient photons appear at different locations in the graph relative to the point of interest, and they can be averaged out.\n\nProblem: inconsistent ambient light\nWhile the authors’ model of computational synchronization is simple and elegant, it relies on the assumption that ambient light stays constant. This can be a problem if the camera, object, or light source is moving around, or there are other light sources other than a constant ambient light. Perhaps SPAD works at a high enough frequency to work under these conditions too. Combining SPAD based 3D cameras with Epiolar ToF methods may be able to mitigate ambient light better.\n",
            "content_html": "<h2 id=\"aka-how-to-deal-with-ambient-light-when-using-spad-as-a-3d-camera\">A.K.A. How to deal with ambient light when using SPAD as a 3D camera</h2>\n<p>Most use cases of SPAD as a 3D camera comes with an inherent problem: ambient light. The 10m-10km measurement range of SPAD cameras means that most uses will be outdoors where ambient light is difficult to control. This paper tackles that by averaging out the ambient light. In contrast to the time of flight graph of a photon to the actual measurement target, which appears as a single high frequency mode in the time-intensity graph, ambient light is a flat low frequency line that stays constant. Because a single SPAD diode can only detect a single photon every so often, the ambient light has a high chance of hitting the diodes and leads to “pileup”. This paper fixes the issue by differing the timestamp at which a laser pulse transmits and one at which a SPAD sensor starts acquiring photons. This way, the pile of ambient photons appear at different locations in the graph relative to the point of interest, and they can be averaged out.</p>\n\n<h2 id=\"problem-inconsistent-ambient-light\">Problem: inconsistent ambient light</h2>\n<p>While the authors’ model of computational synchronization is simple and elegant, it relies on the assumption that ambient light stays constant. This can be a problem if the camera, object, or light source is moving around, or there are other light sources other than a constant ambient light. Perhaps SPAD works at a high enough frequency to work under these conditions too. Combining SPAD based 3D cameras with Epiolar ToF methods may be able to mitigate ambient light better.</p>\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/spad/",
            "date_published": "2023-03-20T00:00:00-04:00",
            "date_modified": "2023-03-20T00:00:00-04:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/epipolar-tof-imaging/",
            "title": "Epipolar Time-of-Flight Imaging",
            "content_text": "\n\nLet’s talk about Achar et al.’s paper “Epipolar Time-of-Flight Imaging”. This relates to my MSCV capstone project “Neural Synthetic Wavelength Interferometry” in that this method and our method both use continuous-wave Time-of-Flight (CW-ToF) sensors. This paper notes CW-ToF sensor’s shortcomings and tries to improve it. In my capstone project, we improve on it by applying neural networks to some parts of the pipeline.\n\nWhat this paper does well\n\nThe strength of this paper is not only its wide applicability, but a thorough consideration for the applications, including eye safety, power consumption, interference &amp; interreflections, and different use cases.\nThe author provide a well explained and useful comparison between epipolar planes’ sampling orders for each scenario. For example, figure 3(e) depicts a scheme that is robust against camera shake (and thus reducing blur in depth map) and samples at twice the speed to fix rolling shutter effect (but sacrificing vertical resolution). Figure 3(f) depicts a scheme assumes a scenario where the sensor is mounted to a car and allows sampling less from the horizon where objects move slowly on the sensor as opposed to near the ground (near the sensor) where things move faster in the projected image.\n\n\n\nQuestions and Exploration\n\n  Can we use a compliant mechanism to move the scanning mirror, to achieve better longevity?\n  Where is the remaining noise coming from?\n  Why are LIDAR sensors expensive, and does this prototype mitigate the high cost?\n  Can we say that epipolar ToF imaging is a generalized version of the usual CW-ToF that uses a 0D laser light source, whereas the epipolar ToF sensor uses 1D lasers? Or is it just LIDAR that uses 0D light sources\n  Is a horizontally sweeping snesor (that uses a vertical epipolar plane) better at mitigating rolling shutter effect on a moving vehicle?\n  Why do the authors use two exposures/readouts per row in figure 5?\n\n\n",
            "content_html": "<p><img src=\"epipolar-setup.jpg\" alt=\"Alt text\" /></p>\n\n<p>Let’s talk about Achar et al.’s paper “Epipolar Time-of-Flight Imaging”. This relates to my MSCV capstone project “Neural Synthetic Wavelength Interferometry” in that this method and our method both use continuous-wave Time-of-Flight (CW-ToF) sensors. This paper notes CW-ToF sensor’s shortcomings and tries to improve it. In my capstone project, we improve on it by applying neural networks to some parts of the pipeline.</p>\n\n<h3 id=\"what-this-paper-does-well\">What this paper does well</h3>\n\n<p>The strength of this paper is not only its wide applicability, but a thorough consideration for the applications, including eye safety, power consumption, interference &amp; interreflections, and different use cases.\nThe author provide a well explained and useful comparison between epipolar planes’ sampling orders for each scenario. For example, figure 3(e) depicts a scheme that is robust against camera shake (and thus reducing blur in depth map) and samples at twice the speed to fix rolling shutter effect (but sacrificing vertical resolution). Figure 3(f) depicts a scheme assumes a scenario where the sensor is mounted to a car and allows sampling less from the horizon where objects move slowly on the sensor as opposed to near the ground (near the sensor) where things move faster in the projected image.</p>\n\n<p><img src=\"sampling-schemes.png\" alt=\"Alt text\" /></p>\n\n<h3 id=\"questions-and-exploration\">Questions and Exploration</h3>\n<ul>\n  <li>Can we use a compliant mechanism to move the scanning mirror, to achieve better longevity?</li>\n  <li>Where is the remaining noise coming from?</li>\n  <li>Why are LIDAR sensors expensive, and does this prototype mitigate the high cost?</li>\n  <li>Can we say that epipolar ToF imaging is a generalized version of the usual CW-ToF that uses a <em>0D</em> laser light source, whereas the epipolar ToF sensor uses <em>1D</em> lasers? Or is it just LIDAR that uses 0D light sources</li>\n  <li>Is a horizontally sweeping snesor (that uses a vertical epipolar plane) better at mitigating rolling shutter effect on a moving vehicle?</li>\n  <li>Why do the authors use two exposures/readouts per row in figure 5?\n<img src=\"timing-diagram.png\" alt=\"Alt text\" /></li>\n</ul>\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/epipolar-tof-imaging/",
            "date_published": "2023-03-01T00:00:00-05:00",
            "date_modified": "2023-03-01T00:00:00-05:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/light-field-camera/",
            "title": "Fourier Slice Photography",
            "content_text": "Let’s talk about Ren Ng’s paper “Fourier Slice Photography”.\n\nThis method is great.\nThis paper combines two ideas. It takes the Fourier Slice Theorem, which is a theorem that states that a projection of a curve can be done in the Fourier domain, and applies it to the 4D light field. In this case, the 2D slice of the 4D light field is a “refocused” image of our choosing. Mindblowing.\n\n\n\nThe author Ren Ng takes this idea and develops it into two iterations of Lytro’s light field cameras. The company has halted execution in 2018 and I was devastated to find out about it.\n\n\nSome problems\nLet’s hear that again. Ren Ng took this idea and developed it into a product. A hardware product. While there were probably other innovations that came along with the Lytro light field cameras, it seems that the lenticular array was a big part of it. That means that this idea can’t be easily applied to existing cameras. It requires a dedicated camera hardware and an assumably a complex manufacturing process. From the point of view of consumers, however, this results in a lower lateral resolution and a huge price tag, all for one function.\n\nQuestions and Exploration\n\n\n  Is there a limit to the distance that re-focusing can work? I wonder what could go wrong if there is an occluding object right in front of the lens.\n  Is this method also faster for computer generated objects? Is it faster to completely re-render using information of the 3D objects, or would it be faster to save the 4D light field and refocus from that?\n  can we apply this on a moving scene? (either in real life or in a generated environment) It seems probable. This would be very useful in film production since I have experienced multiple close up shots thrown away or be used with unsatisfactory results.\n  If we were to apply it to a moving image, can we still do it while previewing the effect of changing focus in real time? This also seems doable. Although it would definitely take more processing time than getting images from the camera sensor directly.\n\n",
            "content_html": "<p>Let’s talk about Ren Ng’s paper “Fourier Slice Photography”.</p>\n\n<h3 id=\"this-method-is-great\">This method is great.</h3>\n<p>This paper combines two ideas. It takes the Fourier Slice Theorem, which is a theorem that states that a projection of a curve can be done in the Fourier domain, and applies it to the 4D light field. In this case, the 2D slice of the 4D light field is a “refocused” image of our choosing. Mindblowing.</p>\n\n<p><img src=\"/course/16-726-sp23/projects/myunggus/assets/images/Fourier_Slice_Theorem.png\" alt=\"Fourier Slice Theorem\" /></p>\n\n<p>The author Ren Ng takes this idea and develops it into two iterations of Lytro’s light field cameras. The company has halted execution in 2018 and I was devastated to find out about it.\n<img src=\"/course/16-726-sp23/projects/myunggus/assets/images/lytro.jpeg\" alt=\"Lytro camera\" /></p>\n\n<h3 id=\"some-problems\">Some problems</h3>\n<p>Let’s hear that again. Ren Ng took this idea and developed it into a product. A hardware product. While there were probably other innovations that came along with the Lytro light field cameras, it seems that the lenticular array was a big part of it. That means that this idea can’t be easily applied to existing cameras. It requires a dedicated camera hardware and an assumably a complex manufacturing process. From the point of view of consumers, however, this results in a lower lateral resolution and a huge price tag, all for one function.</p>\n\n<h3 id=\"questions-and-exploration\">Questions and Exploration</h3>\n\n<ul>\n  <li>Is there a limit to the distance that re-focusing can work? I wonder what could go wrong if there is an occluding object right in front of the lens.</li>\n  <li>Is this method also faster for computer generated objects? Is it faster to completely re-render using information of the 3D objects, or would it be faster to save the 4D light field and refocus from that?</li>\n  <li>can we apply this on a moving scene? (either in real life or in a generated environment) It seems probable. This would be very useful in film production since I have experienced multiple close up shots thrown away or be used with unsatisfactory results.</li>\n  <li>If we were to apply it to a moving image, can we still do it while previewing the effect of changing focus in real time? This also seems doable. Although it would definitely take more processing time than getting images from the camera sensor directly.</li>\n</ul>\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/light-field-camera/",
            "date_published": "2023-02-27T00:00:00-05:00",
            "date_modified": "2023-02-27T00:00:00-05:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj2/",
            "title": "Gradient Domain Fusion and Poisson Blending",
            "content_text": "All images except for “Toy Example” section are mine.\n\nWhen we make montage images, instead of simply copy-pasting an image cutout, it would be nice if the cutout can be edited to match the colour of the background. Poisson blending does this step for us.\n\nPoisson Blending\nThe following \\(argmin\\) optimization is the essence of Poisson Blending.\n\n\n$$\\newcommand{\\argmin}{arg min} \\boldsymbol{v} = \\argmin_{\\boldsymbol{v}}\\sum_{i \\in S, j \\in N_i \\cap S}((v_i - v_j) - (s_i - s_j))^2 + \\sum_{i\\in S, j \\in N_i \\cap \\neg S}((v_i - t_j) - (s_i - s_j))^2$$\n\nwhere\n\n\n  \\(s\\text{: source image}\\)\n  \\(t\\text{: target image}\\)\n  \\(v\\text{: combined image, result of blending}\\)\n  \\(S\\text{: area where mask = True}\\)\n  \\(S^c\\text{: area where mask = False}\\)\n  \\(N_i\\text{: the neighbouring points of }i\\)\n\n\nWe start with a source image (usually an object in the foreground, smaller patch) and a target image (usually the background image, the entire scene will be used). We first draw a mask to decide which area of the source we want to paste into the target image.\n\nNext we look at the gradients of the three images: source, target, and combined. If we want the area inside the mask to be well blended, then we should try to match the gradient \\(v'\\) of the new image to the gradient \\(s'\\) of the source image, assuming we care more about the gradients than the actual pixel values.\n\nNear the border of the mask, however, we want the image to resemble both the source and the target image. Specifically, we want to have the same shape as the source image but have the same colour as the target image. So we try to match the pixel value \\(s\\) of the source image to \\(t + s'\\) which is the pixel value of target image \\(t\\) but changed by the gradient of source image \\(s\\).\n\nToy example\nIn this example, we try to reconstruct an image by looking at the gradients of an image and a single pixel. If we can maintain the gradients as well as that single pixel, we should be able to recover the original image.\n\n\n\nBlending Result\n\n\nAt first, I combined the above two images to get the blended result below.\n\n\n\nBut there were two problems here. The first is that the scale of detail in the waves is off. The second is that although I tried to align the horizon in the two images exactly, due to weather conditions the contrast between the sky and sea are different and so the colour of the sky has not blended in very well.\n\n\n\nAfter changin the magnification and cropping out the sky, I got the image above. It is not perfect either – the colour of the rocks blended into the patch. This could have been prevented with a more content-aware crop.\n\nDiscussion\n\n  Horse blended into cliff:\n\n  \n    Fireworks blended into skyline:\n\nThe first two blended images here point to the fact that blending works well if the surrounding texture in both the source and target images have similar roughness in texture and a similar colour. If I were to position the patches differently, it would not have worked so seamlessly.\n\n\n  \n  Person blended into waterfall:\n\nWhile this image above involving my father is my favourite, it is not the most seamless. That is because the level of focus blur is different for the target and source images. However, making the patch very small helps it to go unnoticed for unassuming eyes. \n\n\n  Squirrel blended onto helmet:\n\nI thought this image of a squirrel would have worked very well – and although the blend itself is quite seamless, the colour of the squirrel is very off. I guess what I thought were two similar colours in the sky and in the background of the squirrel were actually quite different. It seems that to match the two colours, the colour of the squirrel was sacrificed.\n\n\n\n\n\n\n\n\n",
            "content_html": "<p>All images except for “Toy Example” section are mine.</p>\n\n<p>When we make montage images, instead of simply copy-pasting an image cutout, it would be nice if the cutout can be edited to match the colour of the background. Poisson blending does this step for us.</p>\n\n<h2 id=\"poisson-blending\">Poisson Blending</h2>\n<p>The following \\(argmin\\) optimization is the essence of Poisson Blending.</p>\n\n<div>\n$$\\newcommand{\\argmin}{arg min} \\boldsymbol{v} = \\argmin_{\\boldsymbol{v}}\\sum_{i \\in S, j \\in N_i \\cap S}((v_i - v_j) - (s_i - s_j))^2 + \\sum_{i\\in S, j \\in N_i \\cap \\neg S}((v_i - t_j) - (s_i - s_j))^2$$\n</div>\n<p>where</p>\n\n<ul>\n  <li>\\(s\\text{: source image}\\)</li>\n  <li>\\(t\\text{: target image}\\)</li>\n  <li>\\(v\\text{: combined image, result of blending}\\)</li>\n  <li>\\(S\\text{: area where mask = True}\\)</li>\n  <li>\\(S^c\\text{: area where mask = False}\\)</li>\n  <li>\\(N_i\\text{: the neighbouring points of }i\\)</li>\n</ul>\n\n<p>We start with a source image (usually an object in the foreground, smaller patch) and a target image (usually the background image, the entire scene will be used). We first draw a mask to decide which area of the source we want to paste into the target image.</p>\n\n<p>Next we look at the gradients of the three images: source, target, and combined. If we want the area inside the mask to be well blended, then we should try to match the gradient \\(v'\\) of the new image to the gradient \\(s'\\) of the source image, assuming we care more about the gradients than the actual pixel values.</p>\n\n<p>Near the border of the mask, however, we want the image to resemble both the source and the target image. Specifically, we want to have the same <strong>shape</strong> as the source image but have the same <strong>colour</strong> as the target image. So we try to match the pixel value \\(s\\) of the source image to \\(t + s'\\) which is the pixel value of target image \\(t\\) but changed by the gradient of source image \\(s\\).</p>\n\n<h2 id=\"toy-example\">Toy example</h2>\n<p>In this example, we try to reconstruct an image by looking at the gradients of an image and a single pixel. If we can maintain the gradients as well as that single pixel, we should be able to recover the original image.</p>\n\n<p><img src=\"data/toy.png\" alt=\"Alt text\" /></p>\n\n<h2 id=\"blending-result\">Blending Result</h2>\n<p><img src=\"data/jeju_cliff_medium.jpg\" alt=\"Alt text\" width=\"384\" />\n<img src=\"data/dodubong.jpg\" alt=\"Alt text\" width=\"384\" />\nAt first, I combined the above two images to get the blended result below.</p>\n\n<p><img src=\"data/00%20jeju-cliff-lighthouse.png\" alt=\"Alt text\" /></p>\n\n<p>But there were two problems here. The first is that the scale of detail in the waves is off. The second is that although I tried to align the horizon in the two images exactly, due to weather conditions the contrast between the sky and sea are different and so the colour of the sky has not blended in very well.</p>\n\n<p><img src=\"data/00%20dodubong-jeju_cliff.png\" alt=\"Alt text\" /></p>\n\n<p>After changin the magnification and cropping out the sky, I got the image above. It is not perfect either – the colour of the rocks blended into the patch. This could have been prevented with a more content-aware crop.</p>\n\n<h2 id=\"discussion\">Discussion</h2>\n<ol>\n  <li>Horse blended into cliff:\n<img src=\"data/horse+cliff.png\" alt=\"Alt text\" /></li>\n  <li>\n    <p>Fireworks blended into skyline:\n<img src=\"data/00%20fireworks-abudhabi.png\" alt=\"Alt text\" />\nThe first two blended images here point to the fact that blending works well if the surrounding texture in both the source and target images have similar roughness in texture and a similar colour. If I were to position the patches differently, it would not have worked so seamlessly.\n<br />\n<br /></p>\n  </li>\n  <li>Person blended into waterfall:\n<img src=\"data/00%20father+rain_rock.png\" alt=\"Alt text\" />\nWhile this image above involving my father is my favourite, it is not the most seamless. That is because the level of focus blur is different for the target and source images. However, making the patch very small helps it to go unnoticed for unassuming eyes. \n<br />\n<br /></li>\n  <li>Squirrel blended onto helmet:\n<img src=\"data/squirrel+window_cleaner.png\" alt=\"Alt text\" />\nI thought this image of a squirrel would have worked very well – and although the blend itself is quite seamless, the colour of the squirrel is very off. I guess what I thought were two similar colours in the sky and in the background of the squirrel were actually quite different. It seems that to match the two colours, the colour of the squirrel was sacrificed.</li>\n</ol>\n\n<script src=\"https://polyfill.io/v3/polyfill.min.js?features=es6\"></script>\n\n<script id=\"MathJax-script\" async=\"\" src=\"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"></script>\n\n<!-- <script type=\"text/javascript\" src=\"/course/16-726-sp23/projects/myunggus/assets/js/MathJax/MathJax.js\"></script> -->\n<!-- <script type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js\"></script> -->\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj2/",
            "date_published": "2023-02-18T00:00:00-05:00",
            "date_modified": "2023-02-18T00:00:00-05:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj1/",
            "title": "Colorizing the Prokudin-Gorskii Photo Collection",
            "content_text": "Happy Valentine’s Day everyone. A little background on this photo collection and an overview of what this post is about (taken from course homepage):\n\nBackground\nSergei Mikhailovich Prokudin-Gorskii (1863-1944) [Сергей Михайлович Прокудин-Горский, to his Russian friends] was a man well ahead of his time. Convinced, as early as 1907, that color photography was the wave of the future, he won Tzar’s special permission to travel across the vast Russian Empire and take color photographs of everything he saw including the only color portrait of Leo Tolstoy. And he really photographed everything: people, buildings, landscapes, railroads, bridges… thousands of color pictures! His idea was simple: record three exposures of every scene onto a glass plate using a red, a green and a blue filter. Never mind that there was no way to print color photographs until much later – he envisioned special projectors to be installed in “multimedia” classrooms all across Russia where the children would be able to learn about their vast country. Alas, his plans never materialized: he left Russia in 1918, right after the revolution, never to return again. Luckily, his RGB glass plate negatives, capturing the last years of the Russian Empire, survived and were purchased in 1948 by the Library of Congress. The LoC has recently digitized the negatives and made them available on-line.\n\nOverview\nThe goal of this assignment is to take the digitized Prokudin-Gorskii glass plate images and, using image processing techniques, automatically produce a color image with as few visual artifacts as possible. In order to do this, you will need to extract the three color channel images, place them on top of each other, and align them so that they form a single RGB color image. A cool explanation on how the Library of Congress created the color images on their site is available here.\n\nStacking\n\nThe simplest approach to this problem is splitting the image into three equal parts and layering each part as a separate channel. The “dumb” stacking approach is a single line of code but it already outputs something that looks pretty cool.\n\nProblem: Alignment\n\nShifting artifacts\n\nWe see shifts as patches of strong RGB (bright spots) or strong CMY (dark spots). For example, in three_generations, hats have shifted patches with strong yellow at the bottom, strong magenta in the middle, and strong cyan at the top. This is due to the hat being a dark region. Since the surrounding is very bright, a dark patch in the blue channel leads to a relatively bright red and green pixel that shows up as a yellow patch.\n\n\nOn the other hand, the belt is brighter and shows strong red, green, blue patches. \n\n\nSearch space\n\nThe screenshot shows me measuring the vertical displacement between the red and blue channel. From this eyeball measurement, I decided I’ll search over a 160x160 space of possible shifts to align each channel. However, this leads to (10 images) x (2 channel pairs) x (160x160 displacements) = 512,000, which is a little too much. For now, I’ll scale down the images by a factor of four and make it more manageable and only search over 40x40 displacements.\n\nSolution\nSSD\nUsing this method, SSD already gives reasonable results. Examples: Cathedral and Three Generations\n\nDumb Stacking:\n\n\n\nSSD:\n\n\n\nA failure case is with Harvesters where the iterating pattern of the bushes seem to have matched up well but not in other areas. It would be interesting to see if that area has a very low SSD score.\n\n\nNCC\nNCC works slightly better than SSD across most images.\n\nDumb stack:\n\nSSD:\n\nNCC:\n\n\nImage Pyramid\nUsing image pyramids I was able to search a larger space but faster using coarse-to-fine strategy. Combined with cropping, to search across the same space, it takes only about 10% of the time it requires without the image pyramid.\n\nImage Pyramid Height\nDepending on the height/depth of the image pyramid, the high resolution features can get blurred out too much and finds a bad starting point. With a bad starting point, the coarse-to-fine strategy can get stuck at a local minimum. Since the image sizes differed, I used a log function to set the minimum image size to be around 16~32 pixels.\n\nAn extension of this method could also consider average local variability. We can stop downsampling when a target lower bound on the local variability is reached.\n\nLetterbox\n\nNotice that the edges (“letterbox”) are aligned very well. Because the letterbox is so prominent (ie. high contrast/clarity from surrounding areas), properly aligning the letterbox gives the algorithms a good score.\n\nManual Cropping\nI added a manual crop of 10% from each side and kept the 80% as input to the alignment algorithms, to get rid of the effect of the borders. We can see that horizontal alignment has improved a lot, but vertical alignment seems similar. The effect is demonstrated well in train\n\nWithout cropping before input into algorithm:\n \nWith cropping before input into algorithm:\n\n\nI kept reducing the crop factor all the way down to 5%, using only the data in the center of the image to align the channels. The result is a huge speed up and for some images like Emir, this works well because incidentally the center pixels were best for aligning. However, for some images like Lady, the center pixels lack detail and needed a larger window of at least 15% crop factor to achieve good results.\n\nMirroring at the edge\nI’ve used np.roll to maintain the shape of the channels, but the problem with this is that it wraps the edges around to the other side. Edges on different ends of the image may differ greatly in value. This means that the images might be harshly penalized if the frame needs to wrap around.\n\nTo mitigate this possibility, instead of rolling pixels over when shifting the window, I used symmetric padding to ‘extend’ the image rather than wrap it around. (np.pad has modes reflect and symmetric). However, this did not result in better outcomes. It fails miserably in some cases, wrapping around almost halfway. Perhaps the pixels were now too similar to each other that it was hard to find a sensible minimum score.\n\nFinal Results\nNCC, Image Pyramid depth of 3, Crop factor of 0.6\n \n☝ cathedral \t\t\tr_shift=[ 12,  3]\tg_shift=[ 5,  2]\n\nNCC, Image Pyramid depth of 8, Crop factor of 0.6\n \n☝ harvesters  \t\tr_shift=[123, 14]\tg_shift=[59, 17]\n \n☝ icon  \t\t\t\tr_shift=[ 89, 23]\tg_shift=[40, 17]\n \n☝ lady  \t\t\t\tr_shift=[109, 11]\tg_shift=[49,  8]\n \n☝ self_portrait  \tr_shift=[176, 37]\tg_shift=[78, 29]\n \n☝ three_generations  r_shift=[110, 12]\tg_shift=[50, 14]\n \n☝ train  \t\t\tr_shift=[ 87, 32]\tg_shift=[42,  6]\n \n☝ turkmen  \t\t\tr_shift=[116, 28]\tg_shift=[55, 20]\n\nNCC, Image Pyramid depth of 8, Crop factor of 0.1\n \n☝ emir\t\t\t\tr_shift=[105, 41]\tg_shift=[48, 23]\n\n☝ village\t\t\tr_shift=[137, 23]\tg_shift=[64, 13]\n",
            "content_html": "<p>Happy Valentine’s Day everyone. A little background on this photo collection and an overview of what this post is about (taken from course homepage):</p>\n\n<h2 id=\"background\">Background</h2>\n<p><a href=\"http://en.wikipedia.org/wiki/Prokudin-Gorskii\">Sergei Mikhailovich Prokudin-Gorskii</a> (1863-1944) [Сергей Михайлович Прокудин-Горский, to his Russian friends] was a man well ahead of his time. Convinced, as early as 1907, that color photography was the wave of the future, he won Tzar’s special permission to travel across the vast Russian Empire and take color photographs of everything he saw including the only color portrait of <a href=\"http://en.wikipedia.org/wiki/Leo_Tolstoy\">Leo Tolstoy</a>. And he really photographed everything: people, buildings, landscapes, railroads, bridges… thousands of color pictures! His idea was simple: record three exposures of every scene onto a glass plate using a red, a green and a blue filter. Never mind that there was no way to print color photographs until much later – he envisioned special projectors to be installed in “multimedia” classrooms all across Russia where the children would be able to learn about their vast country. Alas, his plans never materialized: he left Russia in 1918, right after the revolution, never to return again. Luckily, his RGB glass plate negatives, capturing the last years of the Russian Empire, survived and were purchased in 1948 by the Library of Congress. The LoC has recently digitized the negatives and made them available on-line.</p>\n\n<h2 id=\"overview\">Overview</h2>\n<p>The goal of this assignment is to take the digitized Prokudin-Gorskii glass plate images and, using image processing techniques, automatically produce a color image with as few visual artifacts as possible. In order to do this, you will need to extract the three color channel images, place them on top of each other, and align them so that they form a single RGB color image. A cool explanation on how the Library of Congress created the color images on their site is available <a href=\"http://www.loc.gov/exhibits/empire/making.html\">here</a>.</p>\n\n<h2 id=\"stacking\">Stacking</h2>\n<p><img src=\"data/dumb-three_generations.jpg\" alt=\"simple stacking three generations\" width=\"384\" />\nThe simplest approach to this problem is splitting the image into three equal parts and layering each part as a separate channel. The “dumb” stacking approach is a single line of code but it already outputs something that looks pretty cool.</p>\n\n<h2 id=\"problem-alignment\">Problem: Alignment</h2>\n\n<h3 id=\"shifting-artifacts\">Shifting artifacts</h3>\n\n<p>We see shifts as patches of strong RGB (bright spots) or strong CMY (dark spots). For example, in <code class=\"language-plaintext highlighter-rouge\">three_generations</code>, hats have shifted patches with strong yellow at the bottom, strong magenta in the middle, and strong cyan at the top. This is due to the hat being a dark region. Since the surrounding is very bright, a dark patch in the blue channel leads to a relatively bright red and green pixel that shows up as a yellow patch.\n<img src=\"data/dumb_three_generations_hat.png\" alt=\"hat\" width=\"384\" /></p>\n\n<p>On the other hand, the belt is brighter and shows strong red, green, blue patches. \n<img src=\"data/dumb_three_generations_belt.png\" alt=\"belt\" width=\"384\" /></p>\n\n<h3 id=\"search-space\">Search space</h3>\n<p><img src=\"data/displacement-measurement.png\" alt=\"Measuring displacement in pixels\" width=\"450\" />\nThe screenshot shows me measuring the vertical displacement between the red and blue channel. From this eyeball measurement, I decided I’ll search over a 160x160 space of possible shifts to align each channel. However, this leads to (10 images) x (2 channel pairs) x (160x160 displacements) = 512,000, which is a little too much. For now, I’ll scale down the images by a factor of four and make it more manageable and only search over 40x40 displacements.</p>\n\n<h2 id=\"solution\">Solution</h2>\n<h3 id=\"ssd\">SSD</h3>\n<p>Using this method, SSD already gives reasonable results. Examples: <code class=\"language-plaintext highlighter-rouge\">Cathedral</code> and <code class=\"language-plaintext highlighter-rouge\">Three Generations</code></p>\n\n<p>Dumb Stacking:\n<img src=\"data/dumb_cathedral.jpg\" alt=\"Alt text\" width=\"384\" />\n<img src=\"data/dumb-three_generations.jpg\" alt=\"Alt text\" width=\"384\" /></p>\n\n<p>SSD:\n<img src=\"data/ssd_cathedral.jpg\" alt=\"Alt text\" width=\"384\" />\n<img src=\"data/ssd_three_generations.jpg\" alt=\"Alt text\" width=\"384\" /></p>\n\n<p>A failure case is with <code class=\"language-plaintext highlighter-rouge\">Harvesters</code> where the iterating pattern of the bushes seem to have matched up well but not in other areas. It would be interesting to see if that area has a very low SSD score.\n<img src=\"data/ssd-harvesters.jpg\" alt=\"harvesters\" width=\"384\" /></p>\n\n<h3 id=\"ncc\">NCC</h3>\n<p>NCC works slightly better than SSD across most images.</p>\n\n<p>Dumb stack:\n<img src=\"data/dumb_cathedral.jpg\" alt=\"Alt text\" width=\"384\" />\nSSD:\n<img src=\"data/ssd_cathedral.jpg\" alt=\"Alt text\" width=\"384\" />\nNCC:\n<img src=\"data/ncc_cathedral.jpg\" alt=\"Alt text\" width=\"384\" /></p>\n\n<h3 id=\"image-pyramid\">Image Pyramid</h3>\n<p>Using image pyramids I was able to search a larger space but faster using coarse-to-fine strategy. Combined with cropping, to search across the same space, it takes only about 10% of the time it requires without the image pyramid.</p>\n\n<h4 id=\"image-pyramid-height\">Image Pyramid Height</h4>\n<p>Depending on the height/depth of the image pyramid, the high resolution features can get blurred out too much and finds a bad starting point. With a bad starting point, the coarse-to-fine strategy can get stuck at a local minimum. Since the image sizes differed, I used a log function to set the minimum image size to be around 16~32 pixels.</p>\n\n<p>An extension of this method could also consider average local variability. We can stop downsampling when a target lower bound on the local variability is reached.</p>\n\n<h3 id=\"letterbox\">Letterbox</h3>\n\n<p>Notice that the edges (“letterbox”) are aligned very well. Because the letterbox is so prominent (ie. high contrast/clarity from surrounding areas), properly aligning the letterbox gives the algorithms a good score.</p>\n\n<h4 id=\"manual-cropping\">Manual Cropping</h4>\n<p>I added a manual crop of 10% from each side and kept the 80% as input to the alignment algorithms, to get rid of the effect of the borders. We can see that horizontal alignment has improved a lot, but vertical alignment seems similar. The effect is demonstrated well in <code class=\"language-plaintext highlighter-rouge\">train</code></p>\n\n<p>Without cropping before input into algorithm:\n<img src=\"data/train_ncc.jpg\" alt=\"Train NCC\" width=\"384\" /> \nWith cropping before input into algorithm:\n<img src=\"data/train_ncc_manual-crop.jpg\" alt=\"Train NCC Cropped\" width=\"384\" /></p>\n\n<p>I kept reducing the crop factor all the way down to 5%, using only the data in the center of the image to align the channels. The result is a huge speed up and for some images like <code class=\"language-plaintext highlighter-rouge\">Emir</code>, this works well because incidentally the center pixels were best for aligning. However, for some images like <code class=\"language-plaintext highlighter-rouge\">Lady</code>, the center pixels lack detail and needed a larger window of at least 15% crop factor to achieve good results.</p>\n\n<h4 id=\"mirroring-at-the-edge\">Mirroring at the edge</h4>\n<p>I’ve used <code class=\"language-plaintext highlighter-rouge\">np.roll</code> to maintain the shape of the channels, but the problem with this is that it wraps the edges around to the other side. Edges on different ends of the image may differ greatly in value. This means that the images might be harshly penalized if the frame needs to wrap around.</p>\n\n<p>To mitigate this possibility, instead of rolling pixels over when shifting the window, I used symmetric padding to ‘extend’ the image rather than wrap it around. (<code class=\"language-plaintext highlighter-rouge\">np.pad</code> has modes <code class=\"language-plaintext highlighter-rouge\">reflect</code> and <code class=\"language-plaintext highlighter-rouge\">symmetric</code>). However, this did not result in better outcomes. It fails miserably in some cases, wrapping around almost halfway. Perhaps the pixels were now too similar to each other that it was hard to find a sensible minimum score.</p>\n\n<h3 id=\"final-results\">Final Results</h3>\n<h4 id=\"ncc-image-pyramid-depth-of-3-crop-factor-of-06\">NCC, Image Pyramid depth of 3, Crop factor of 0.6</h4>\n<p><img src=\"data/final/cathedral.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">cathedral \t\t\tr_shift=[ 12,  3]\tg_shift=[ 5,  2]</code></p>\n\n<h4 id=\"ncc-image-pyramid-depth-of-8-crop-factor-of-06\">NCC, Image Pyramid depth of 8, Crop factor of 0.6</h4>\n<p><img src=\"data/final/harvesters.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">harvesters  \t\tr_shift=[123, 14]\tg_shift=[59, 17]</code>\n<img src=\"data/final/icon.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">icon  \t\t\t\tr_shift=[ 89, 23]\tg_shift=[40, 17]</code>\n<img src=\"data/final/lady.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">lady  \t\t\t\tr_shift=[109, 11]\tg_shift=[49,  8]</code>\n<img src=\"data/final/self_portrait.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">self_portrait  \tr_shift=[176, 37]\tg_shift=[78, 29]</code>\n<img src=\"data/final/three_generations.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">three_generations  r_shift=[110, 12]\tg_shift=[50, 14]</code>\n<img src=\"data/final/train.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">train  \t\t\tr_shift=[ 87, 32]\tg_shift=[42,  6]</code>\n<img src=\"data/final/turkmen.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">turkmen  \t\t\tr_shift=[116, 28]\tg_shift=[55, 20]</code></p>\n\n<h4 id=\"ncc-image-pyramid-depth-of-8-crop-factor-of-01\">NCC, Image Pyramid depth of 8, Crop factor of 0.1</h4>\n<p><img src=\"data/final/emir.jpg\" alt=\"label\" /> \n☝ <code class=\"language-plaintext highlighter-rouge\">emir\t\t\t\tr_shift=[105, 41]\tg_shift=[48, 23]</code>\n<img src=\"data/final/village.jpg\" alt=\"label\" />\n☝ <code class=\"language-plaintext highlighter-rouge\">village\t\t\tr_shift=[137, 23]\tg_shift=[64, 13]</code></p>\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj1/",
            "date_published": "2023-02-15T00:00:00-05:00",
            "date_modified": "2023-02-15T00:00:00-05:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/helmholtz-stereopsis/",
            "title": "Helmholtz Stereopsis",
            "content_text": "Let’s talk about Zickler et al.’s paper “Helmholtz Stereopsis: Exploiting Reciprocity for Surface Reconstruction”. The name ‘Helmholtz Stereopsis’ comes from the fact that this method exploits Helmholtz reciprocity. It’s a physical property of most material that says that swapping the location of lighting and camera will not change the brightness of a point on the material.\n\nThis method is great.\nJumping to conclusions: Helmholtz stereopsis does better than two pre-existing general stereopsis techniques (photometric and conventional multinocular) in almost every possible way. The most useful of its premises is that it assumes nothing about the BRDF (Bidirectional Reflectance Distribution Function) so it can be used on a wide range of objects. The most useful of its abilities is that it can retrieve depth and surface normals at the same time. \nWhy is it important that we can get depth and surface normals? Because calculating one from the other often leads to error and/or may need fancy math to do it properly.\n\nIdea\nA limitation of this method is that it cannot usually be used on larger objects or fixed objects outdoors that cannot be put under controlled lighting settings. However, there is one light source outdoors that constantly changes – the Sun. We know the precise location of the Sun at a given time and we could also program a drone to capture images from a precise location and angle. Using Helmholtz Stereopsis could work in our favour when we’re modelling a large landscape - maybe a mountain or a building. Urban landscape modelling could be useful for mobile map services to provide rough 3D shapes of buildings (especially complex ones like Gates-Hillman Center). Within a short time frame, we can assume the direction of light from the Sun is constant and parallel. Then an aerial drone can fix its camera angle and capture multiple shots from different positions. (Does distance of the camera from the scene matter here under perspective projection?)\n",
            "content_html": "<p>Let’s talk about Zickler et al.’s paper “Helmholtz Stereopsis: Exploiting Reciprocity for Surface Reconstruction”. The name ‘Helmholtz Stereopsis’ comes from the fact that this method exploits Helmholtz reciprocity. It’s a physical property of most material that says that swapping the location of lighting and camera will not change the brightness of a point on the material.</p>\n\n<h3 id=\"this-method-is-great\">This method is great.</h3>\n<p>Jumping to conclusions: Helmholtz stereopsis does better than two pre-existing general stereopsis techniques (photometric and conventional multinocular) in almost every possible way. The most useful of its premises is that it assumes nothing about the BRDF (Bidirectional Reflectance Distribution Function) so it can be used on a wide range of objects. The most useful of its abilities is that it can retrieve depth <em>and</em> surface normals at the same time. \nWhy is it important that we can get depth and surface normals? Because calculating one from the other often leads to error and/or may need fancy math to do it properly.</p>\n\n<h3 id=\"idea\">Idea</h3>\n<p>A limitation of this method is that it cannot usually be used on larger objects or fixed objects outdoors that cannot be put under controlled lighting settings. However, there is one light source outdoors that constantly changes – the Sun. We know the precise location of the Sun at a given time and we could also program a drone to capture images from a precise location and angle. Using Helmholtz Stereopsis could work in our favour when we’re modelling a large landscape - maybe a mountain or a building. Urban landscape modelling could be useful for mobile map services to provide rough 3D shapes of buildings (especially complex ones like Gates-Hillman Center). Within a short time frame, we can assume the direction of light from the Sun is constant and parallel. Then an aerial drone can fix its camera angle and capture multiple shots from different positions. (Does distance of the camera from the scene matter here under perspective projection?)</p>\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/helmholtz-stereopsis/",
            "date_published": "2023-02-07T00:00:00-05:00",
            "date_modified": "2023-02-07T00:00:00-05:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/random/",
            
            "content_text": "I won’t use title headings for normal posts. Let’s see how long the excerpt goes on for. This is a pretty long paragraph and I expect it should have been cut off by now. However, this is another sentence just in case.\n\n\n",
            "content_html": "<p>I won’t use title headings for normal posts. Let’s see how long the excerpt goes on for. This is a pretty long paragraph and I expect it should have been cut off by now. However, this is another sentence just in case.</p>\n\n<!-- excerpt_separator -->\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/random/",
            "date_published": "2023-01-23T00:00:00-05:00",
            "date_modified": "2023-01-23T00:00:00-05:00",
            "author": {
                "name": "Simon Seo"
            }
        },
        
    
      
        
        
        {
            "id": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj0/",
            "title": "#0: How to submit assignments?",
            "content_text": "The question that opens a course at CMU. Perhaps the only thing I can figure out perfectly in this course.\n\n\n\nThe first paragraph is not indented. The second paragraph and onwards are indented.\n\nThis website theme was made by patdryburgh. Photos below are mine though.\n\n\n\n  \n  \n  \n    Photos by myunggus\n  \n\n\nWe have code highlighting\n\n# python\nclass Example:\n    pass\n\n\nAnd to produce a page like this we need the following folder structure:\n\n_posts &gt; whatever_folder_name &gt; yyyy-mm-dd-projN.md\n_posts &gt; whatever_folder_name &gt; data &gt; whatever_local_files\n\nwhere N is a number.\n\nThey only need a filename.\nPermalink in the front-matter of post-title.md should be empty.\nThe date in the front-matter is what actually gets used.\n",
            "content_html": "<p>The question that opens a course at CMU. Perhaps the only thing I can figure out perfectly in this course.</p>\n\n<!-- excerpt_separator -->\n\n<p>The first paragraph is not indented. The second paragraph and onwards are indented.</p>\n\n<p>This website theme was made by <a href=\"https://patdryburgh.com/\">patdryburgh</a>. Photos below are mine though.</p>\n\n<p><img src=\"data/output_bezier_1850.jpg\" alt=\"bezier curves\" /></p>\n<figure>\n  <img alt=\"\" src=\"/course/16-726-sp23/projects/myunggus/assets/images/DSC09736 output_rotatedrect_16.jpg\" />\n  <img alt=\"\" src=\"/course/16-726-sp23/projects/myunggus/assets/images/DSC09736.jpg\" />\n  <figcaption>\n    Photos by myunggus\n  </figcaption>\n</figure>\n\n<p>We have code highlighting</p>\n\n<div class=\"language-py highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code><span class=\"c1\"># python\n</span><span class=\"k\">class</span> <span class=\"nc\">Example</span><span class=\"p\">:</span>\n    <span class=\"k\">pass</span>\n</code></pre></div></div>\n\n<p>And to produce a page like this we need the following folder structure:</p>\n\n<div class=\"language-plaintext highlighter-rouge\"><div class=\"highlight\"><pre class=\"highlight\"><code>_posts &gt; whatever_folder_name &gt; yyyy-mm-dd-projN.md\n_posts &gt; whatever_folder_name &gt; data &gt; whatever_local_files\n</code></pre></div></div>\n<p>where N is a number.</p>\n\n<p>They only need a filename.\nPermalink in the front-matter of post-title.md should be empty.\nThe date in the front-matter is what actually gets used.</p>\n",
            "url": "http://localhost:4000/course/16-726-sp23/projects/myunggus/proj0/",
            "date_published": "2023-01-22T00:00:00-05:00",
            "date_modified": "2023-01-22T00:00:00-05:00",
            "author": {
                "name": "Simon Seo"
            }
        }
        
    
    ]
}