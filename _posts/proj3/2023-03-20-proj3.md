---
title: "When Cats meet GANs"
layout: post
author: "Simon Seo"
categories: assignment
date: 2023-03-12
toc: true
---

We will talk about Deep Convolutional GANs (DCGAN) and CycleGANs. 

## Goal

1. Generate cat pictures from a randomly sampled noise vector in the latent/feature space.
1. Translate between images of two different types of cats (grumpy and Russian Blue)
1. Translate between images of apples and images of oranges

##1. Deep Convolutional GAN

DCGAN is just GAN but with convolutional layers. 

### Deep Convolutional Discriminator Padding
![dcgan discriminator architecture](data/discriminator_architecture.png)

The above image (taken from course website) describes the layers in the discriminator but we need to determine the sizes of the kernel, stride, and padding. We downsample the image by a factor of two using the following formula.

<div>
$$
(O \times S)+(K-S) = (I+2P) \\ 
O = I /2 
$$
</div>

Where O is output size, I is input size, K is kernel size, S is stride, and P is padding. We are given kernel size K=4, stride S=2. When S is equal to the downsampling factor, the actual values of O and I become irrelevant and we get P=1 for all layers except `conv5`, where we don't need padding nor a stride if we use a kernel size K=4.


### Deep Convolutional Generator Up-convolution

![generator architecture](data/DCGAN%20generator_architecture.png)

The first layer of the generator is given input of size 1x1 which needs to be quadrupled to 4x4. Through multiple experiments, I found that using a kernel size K=5, stride S=1, padding P=2 after scaling the image to the desired size made the training more stable.

### Preprocess Augmentation and Differentiable Augmentation

I used all three DiffAug policies `color,translation,cutout` and in deluxe data augmentation, I used `RandomCrop`, `RandomHorizontalFlip`, `RandomRotation` transforms on top of the basic data augmentation transforms. 

I applied DiffAug to both fake and real images any time the images needed to go through the discriminator. I also called `.detach()` method on fake images only when training the discriminator to fix the generator.

vanilla gan grumpy cat basic: 
![vanilla gan grumpy cat basic ](data/vanilla%20grumpy%20basic%206400.png) 
vanilla gan grumpy cat basic diffaug:
![vanilla gan grumpy cat basic diffaug](data/vanilla%20grumpy%20basic%20diffaug%206400.png) 
vanilla gan grumpy cat deluxe: 
![vanilla gan grumpy cat deluxe ](data/vanilla%20grumpy%20deluxe%206400.png) 
vanilla gan grumpy cat deluxe diffaug:
![vanilla gan grumpy cat deluxe diffaug](data/vanilla%20grumpy%20deluxe%20diffaug%206400.png)

The effects that I can observe from running 6400 iterations with these different settings are:
- Data augmentation helps. Applying either `deluxe` or `diffaug` significantly enhances the image quality.
- `diffaug` seems to work better than plain `deluxe`. Perhaps because the effect of `diffaug` is only applied to the discriminator. With the `deluxe` option, the generator must learn to generalize a broader range of data, which may have been difficult with just 6400 iterations.
- In the case that `diffaug` was applied, the deluxe augmentations allow more variance in the output images but the results are not as clean as the basic augmented output. Running for longer may have produced a different outcome. 



--- 
All images except for "Toy Example" section are mine.

When we make montage images, instead of simply copy-pasting an image cutout, it would be nice if the cutout can be edited to match the colour of the background. Poisson blending does this step for us.

## Poisson Blending
The following $$argmin$$ optimization is the essence of Poisson Blending.

<div>
$$\newcommand{\argmin}{arg min} \boldsymbol{v} = \argmin_{\boldsymbol{v}}\sum_{i \in S, j \in N_i \cap S}((v_i - v_j) - (s_i - s_j))^2 + \sum_{i\in S, j \in N_i \cap \neg S}((v_i - t_j) - (s_i - s_j))^2$$
</div>
where


- \\(s\text{: source image}\\)
- \\(t\text{: target image}\\)
- \\(v\text{: combined image, result of blending}\\)
- \\(S\text{: area where mask = True}\\)
- \\(S^c\text{: area where mask = False}\\)
- \\(N_i\text{: the neighbouring points of }i\\)

We start with a source image (usually an object in the foreground, smaller patch) and a target image (usually the background image, the entire scene will be used). We first draw a mask to decide which area of the source we want to paste into the target image. 

Next we look at the gradients of the three images: source, target, and combined. If we want the area inside the mask to be well blended, then we should try to match the gradient $$v'$$ of the new image to the gradient $$s'$$ of the source image, assuming we care more about the gradients than the actual pixel values. 

Near the border of the mask, however, we want the image to resemble both the source and the target image. Specifically, we want to have the same **shape** as the source image but have the same **colour** as the target image. So we try to match the pixel value $$s$$ of the source image to $$t + s'$$ which is the pixel value of target image $$t$$ but changed by the gradient of source image $$s$$.

## Toy example
In this example, we try to reconstruct an image by looking at the gradients of an image and a single pixel. If we can maintain the gradients as well as that single pixel, we should be able to recover the original image.

![Alt text](data/toy.png)

## Blending Result
![Alt text](data/jeju_cliff_medium.jpg){:width="384"}
![Alt text](data/dodubong.jpg){:width="384"}
At first, I combined the above two images to get the blended result below.

![Alt text](data/00%20jeju-cliff-lighthouse.png)

But there were two problems here. The first is that the scale of detail in the waves is off. The second is that although I tried to align the horizon in the two images exactly, due to weather conditions the contrast between the sky and sea are different and so the colour of the sky has not blended in very well. 

![Alt text](data/00%20dodubong-jeju_cliff.png)

After changin the magnification and cropping out the sky, I got the image above. It is not perfect either -- the colour of the rocks blended into the patch. This could have been prevented with a more content-aware crop. 

## Discussion
1. Horse blended into cliff:
![Alt text](data/horse+cliff.png)
1. Fireworks blended into skyline:
![Alt text](data/00%20fireworks-abudhabi.png)
The first two blended images here point to the fact that blending works well if the surrounding texture in both the source and target images have similar roughness in texture and a similar colour. If I were to position the patches differently, it would not have worked so seamlessly.
<br>
<br>

1. Person blended into waterfall:
![Alt text](data/00%20father+rain_rock.png)
While this image above involving my father is my favourite, it is not the most seamless. That is because the level of focus blur is different for the target and source images. However, making the patch very small helps it to go unnoticed for unassuming eyes. 
<br>
<br>
1. Squirrel blended onto helmet:
![Alt text](data/squirrel+window_cleaner.png)
I thought this image of a squirrel would have worked very well -- and although the blend itself is quite seamless, the colour of the squirrel is very off. I guess what I thought were two similar colours in the sky and in the background of the squirrel were actually quite different. It seems that to match the two colours, the colour of the squirrel was sacrificed.


<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
<!-- <script type="text/javascript" src="{{ site.baseurl }}/assets/js/MathJax/MathJax.js"></script> -->
<!-- <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js"></script> -->